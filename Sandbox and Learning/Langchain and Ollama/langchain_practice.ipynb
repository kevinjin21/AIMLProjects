{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOllama Using pre-loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.smith.langchain.com'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies - general\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('.env')\n",
    "os.environ['LANGCHAIN_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies - chatollama, chat templates\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage # overloaded from BaseMessage\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# grabbing the model (local, see https://github.com/ollama/ollama/blob/main/docs/api.md for full docs)\n",
    "base_url = 'http://localhost:11434'\n",
    "# model = 'llama3.2:latest'\n",
    "model = 'sheldon'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model,\n",
    "    base_url=base_url,\n",
    "    temperature=0.8,\n",
    "    num_predict=256,\n",
    "    # other params ..\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Finally, someone has deigned to acknowledge my presence. I do hope you're aware that I was in the middle of a critical experiment and had to pause it due to your interruption. Now, if you'd like to discuss the intricacies of String theory or my expertise on comic books, I'm more than happy to enlighten you. Otherwise, please, state your purpose.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke('hi')\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Messages\n",
    "SystemMessage - pass role of LLM in run-time; can direct how the response should be generated.\n",
    "\\\n",
    "HumanMessage - human prompt input, user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage # overloaded from BaseMessage\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three important things to know about our planet:\n",
      "\n",
      "1. The Earth is a big ball that floats in space and is home to all kinds of living things.\n",
      "2. Our Earth has air, water, and land, which we need to survive.\n",
      "3. We must take care of the Earth so it stays healthy and happy for us and future generations!\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage('Tell me about the Earth in 3 points.') # your prompt/question to ask\n",
    "system = SystemMessage('You are an elementary school teacher. You should answer in short, easy to understand sentences.') # system description - how to respond\n",
    "\n",
    "messages = [system, question] # feed into llm to generate response\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A most intriguing inquiry! As a Ph.D. Professor of Geoscience, I'm delighted to provide you with three salient points regarding our terrestrial home, the Earth.\n",
      "\n",
      "1. **Planetary Composition and Structure**: The Earth is a terrestrial planet composed of approximately 31% iron, 30% calcium carbonate (limestone), 10% silicates (minerals), and 6% other elements such as aluminum, magnesium, and potassium. Its crust ranges in thickness from 5-70 km, with the majority of the solid mass residing within its mantle. The Earth's core is divided into a molten outer layer (core mantle boundary) and a solid inner core, comprising approximately 20% of the planet's mass.\n",
      "2. **Atmospheric Dynamics and Climate Regulation**: The Earth's atmosphere is a dynamic system that plays a crucial role in regulating our climate and supporting life. Comprising 78% nitrogen, 21% oxygen, and smaller percentages of other gases, the atmosphere is held in place by gravity and maintains its pressure through atmospheric pressure gradients. The atmospheric circulation patterns, such as trade winds and jet streams, influence regional climates and weather phenomena.\n",
      "3. **Geological History and Tectonic Evolution**: The Earth's geological history is characterized by a complex tectonic evolution that has shaped the planet over billions of years. From the formation of the Moon (4.5 billion years ago) to the break-up of supercontinents like Pangaea (~300 million years ago), the Earth has undergone numerous episodes of mountain-building, rifting, and volcanic activity. These processes have sculpted the surface topography and created a rich geological record that provides insights into the planet's evolution.\n",
      "\n",
      "These three points offer a glimpse into the fascinating complexity of our terrestrial home, the Earth.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessage('You a phd professor. You should answer as such.') # showing difference caused by system prompt\n",
    "\n",
    "messages = [system, question] \n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "Replaces manually writing the system message, instead can be used as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up templates. We can later fill in variables to provide flexibility.\n",
    "system = SystemMessagePromptTemplate.from_template('You are a {school} teacher. You should answer in short, easy to understand sentences.')\n",
    "question = HumanMessagePromptTemplate.from_template('Tell me about {topic} in {number} points.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='You are a high school teacher. You should answer in short, easy to understand sentences.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.format(topic='basketball', number='3')\n",
    "system.format(school='high school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['number', 'school', 'topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are a {school} teacher. You should answer in short, easy to understand sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['number', 'topic'], input_types={}, partial_variables={}, template='Tell me about {topic} in {number} points.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points about basketball:\n",
      "\n",
      "1. Basketball is a team sport played on a court with two teams of five players each.\n",
      "2. The objective is to score more points than the opposing team by shooting the ball into their goal, called a hoop or basket.\n",
      "3. Players can move the ball by dribbling or passing it to teammates, and they have four chances, or shots, to score before the ball is turned over.\n"
     ]
    }
   ],
   "source": [
    "question = template.invoke({'school': 'high school', 'topic': 'basketball', 'number': '3'})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five key points about soccer:\n",
      "\n",
      "1. Soccer is a team sport played with two teams of eleven players each.\n",
      "2. The objective is to score more goals than the opposing team by kicking or heading the ball into their goal.\n",
      "3. Players can use any part of their body except their hands and arms to control and move the ball.\n",
      "4. A match is divided into two 45-minute halves, with a 15-minute halftime break in between.\n",
      "5. The team with the most goals at the end of the two halves wins the game.\n"
     ]
    }
   ],
   "source": [
    "# showing how much faster it can be with templates:\n",
    "question = template.invoke({'school': 'high school', 'topic': 'soccer', 'number': '5'})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language - LCEL chains\n",
    "Types:\n",
    "- Sequential Chain\n",
    "- Parallel Chain\n",
    "- Router Chain\n",
    "- Chain Runnables\n",
    "- Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing previously used templates\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "base_url = 'http://localhost:11434'\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are a {school} teacher. You should answer in short, easy to understand sentences.')\n",
    "question = HumanMessagePromptTemplate.from_template('Tell me about {topic} in {number} points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start off the same way as using chat templates before:\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['number', 'school', 'topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are a {school} teacher. You should answer in short, easy to understand sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['number', 'topic'], input_types={}, partial_variables={}, template='Tell me about {topic} in {number} points.'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2', base_url='http://localhost:11434')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of creating a question using template.invoke, we chain together the template and llm\n",
    "chain = template | llm # creating the chain\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a teacher, not an expert on games! But I can try my best.\n",
      "\n",
      "Here's what I know about poker:\n",
      "\n",
      "1. Poker is a card game where people bet money on their hands.\n",
      "2. The goal is to win the pot by having the best hand or being the last person left in the game.\n",
      "3. There are many types of poker, but most involve making bets and trying to beat other players.\n",
      "\n",
      "Let me know if you'd like more info!\n"
     ]
    }
   ],
   "source": [
    "# then, we can invoke the chain together instead of two separate times (see above chat template section)\n",
    "response = chain.invoke({'school': 'primary', 'number': '3', 'topic': 'poker'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking it a step further, we can add other things to the chain.\n",
    "For example, converting the output to a text string instead of just having a response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points about 401(k):\n",
      "\n",
      "1. A 401(k) is a type of retirement savings plan offered by many employers.\n",
      "2. Contributions made to a 401(k) are typically made before taxes, reducing your taxable income for the year.\n",
      "3. Earnings on your 401(k) investments grow tax-deferred, meaning you won't pay taxes until you withdraw the money in retirement.\n"
     ]
    }
   ],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'school': 'college', 'number': 3, 'topic': '401k'})\n",
    "print(response) # here's the main difference; can directly print the response since it'll be a string now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Runnables\n",
    "We can use the same logic to add more things to the chain and run at once. For this example, we will take the output of the previous chain and perform analysis on it. In other words, the output (response) will be used as the input for another llm call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would rate the difficulty of understanding this text as a 2 out of 5.\n",
      "\n",
      "This rating is based on the fact that the text presents basic and straightforward information about 401k plans, using simple language and short sentences that are easy to comprehend. The concepts are also clearly explained, making it accessible to a general audience.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('''analyze the following text: {text}\n",
    "                                                   Give a rating from 1 to 5 on how difficult it is to understand. \n",
    "                                                   Provide one sentence to explain this rating.\n",
    "                                                   ''')\n",
    "diff_chain = chain | analysis_prompt | llm | StrOutputParser() # chain linked to analysis prompt linked to llm and str output\n",
    "# diff_chain_alt = {'response': chain} | analysis_prompt | llm | StrOutputParser()\n",
    "output = diff_chain.invoke({'school': 'college', 'number': 3, 'topic': '401k'}) # I can directly use the inputs from chain\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Chains\n",
    "Parallel chains are used to run multiple runnables in parallel, the final return is a dict with the result of each value under its appropriate key.\n",
    "\n",
    "The main difference from chaining (in series) is that parallel runnables MUST BE INDEPENDENT, while series runnables can have the input of one be the output of another. For example: explain plot of story -> give rating of plot explanation -> generate star value VS. explain plot of book || explain plot of movie -> using output from both: compare which plot is better explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bread go to therapy?\n",
      "\n",
      "Because it was feeling crumby.\n",
      "==================================\n",
      "Softly baked and fragrant air,\n",
      "Freshly cut, with love to share.\n"
     ]
    }
   ],
   "source": [
    "# start two independent chains\n",
    "joke_chain = ChatPromptTemplate.from_template('tell me a joke about {topic}') | llm | StrOutputParser()\n",
    "poem_chain = ChatPromptTemplate.from_template('write a {number}-line poem about {topic}') | llm | StrOutputParser()\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain) # create a parallel chain with both chains inside\n",
    "\n",
    "# when invoking, ALL inputs need to be provided, even if they are not all shared.\n",
    "output = map_chain.invoke({'topic': 'bread', 'number': 2})\n",
    "print(output['joke'] + '\\n==================================\\n' + output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Router\n",
    "Logic flow tool - uses output of a chain to affect where the chain continues. Based on the response, router can decide next chain.\n",
    "\n",
    "Example using poem/joke chain:\\\n",
    "Given a user review, classify if it is positive or negative. Then, if it is positive, create a joke about the topic. Else, create a poem to console the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being 'Positive' or 'Negative'.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Review: {review}\n",
    "            Movie Name: {title}\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "review_chain = template | llm | StrOutputParser()\n",
    "\n",
    "review = 'This movie blew my socks off. I was at the edge of my seat from start to finish. I loved it!'\n",
    "\n",
    "review_chain.invoke({'review': review, 'title': 'Shark Tale'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a router to perform an action based on review_chain's output\n",
    "def route(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return joke_chain\n",
    "    else:\n",
    "        return poem_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "We will use runnablelambda to select a chain based on the output of review_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: ChatPromptTemplate(input_variables=['review', 'title'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review', 'title'], input_types={}, partial_variables={}, template=\"Given the user review below, classify it as either being 'Positive' or 'Negative'.\\n            Do not respond with more than one word.\\n\\n            Review: {review}\\n            Movie Name: {title}\\n\"), additional_kwargs={})])\n",
       "             | ChatOllama(model='llama3.2', base_url='http://localhost:11434')\n",
       "             | StrOutputParser(),\n",
       "  topic: RunnableLambda(...),\n",
       "  number: RunnableLambda(...)\n",
       "}\n",
       "| RunnableLambda(route)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain = {'sentiment': review_chain, 'topic': lambda x: x['title'], 'number': lambda x: len(x['title']) // 5} | RunnableLambda(route) # store number in case the review is negative\n",
    "full_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Lenny the fish go to the party in Shark Tale?\n",
      "\n",
      "Because he heard it was a \"reel\" good time! (get it?)\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({'review': review, 'title': 'Shark Tale'})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
