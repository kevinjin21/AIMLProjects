{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6dd410",
   "metadata": {},
   "source": [
    "# Testing supervisor agent to fix structure for streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d428508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: float, b: float):\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "base_url = 'http://localhost:11434'\n",
    "model = 'llama3.2'\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[add, multiply, divide],\n",
    "    prompt=(\n",
    "        \"You are a math agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with math-related tasks\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"math_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: math_agent\n",
      "Tool Calls:\n",
      "  multiply (360a174d-0e70-43db-8481-536bcb03b05c)\n",
      " Call ID: 360a174d-0e70-43db-8481-536bcb03b05c\n",
      "  Args:\n",
      "    a: (add 3 5)\n",
      "    b: 7\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "Error: 1 validation error for multiply\n",
      "a\n",
      "  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='(add 3 5)', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/float_parsing\n",
      " Please fix your mistakes.\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: math_agent\n",
      "\n",
      "{\"result\": 56,\"error\": None}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in math_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 7\"}]}\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4b2003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Is Harsha a Devops Enginer?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_person_details (4c42a120-594f-4299-9f64-d7ced0e9544c)\n",
      " Call ID: 4c42a120-594f-4299-9f64-d7ced0e9544c\n",
      "  Args:\n",
      "    person_name: Harsha\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_person_details\n",
      "\n",
      "Harsha is a DevOps Engineer.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "No, I couldn't find any information about an individual named Harsha being a DevOps Engineer. The previous response was based on the tool output which did not provide sufficient information to confirm this. Can you please provide more context or details?\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from typing import Any\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Define the state\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Tool: Person Information\n",
    "def get_person_details(person_name: str) -> str:\n",
    "    \"\"\"Retrieve details of a person.\"\"\"\n",
    "    return f\"{person_name} is a DevOps Engineer.\"\n",
    "\n",
    "\n",
    "# Tool: Location Information\n",
    "def get_person_location(person_name: str) -> str:\n",
    "    \"\"\"Retrieve location of a person.\"\"\"\n",
    "    return f\"{person_name} lives in Bangalore.\"\n",
    "\n",
    "\n",
    "# Initialize the Ollama chat model\n",
    "base_url = 'http://localhost:11434'\n",
    "model = 'llama3.2'\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "# Bind tools to the Groq model\n",
    "tools = [get_person_details, get_person_location]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helpful assistant that provides accurate responses based on the given tools.\")\n",
    "\n",
    "\n",
    "# Define the assistant node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Adding Edge\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Test interaction with memory\n",
    "# Thread ID for maintaining context\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "\n",
    "messages = [HumanMessage(\n",
    "    content=\"Tell me about Harsha?\")]\n",
    "result = react_graph.invoke({\"messages\": messages}, config)\n",
    "\n",
    "\n",
    "messages = [HumanMessage(\n",
    "    content=\"Is Harsha a Devops Enginer?\")]\n",
    "result = react_graph.invoke({\"messages\": messages}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d984412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.foundry_llm import FoundryLLM\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = FoundryLLM(\n",
    "    url=os.environ['FOUNDRY_LLM_URL'],\n",
    "    client_id=os.environ['FND_CLNT_ID'],\n",
    "    client_secret=os.environ['FND_CLNT_SEC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90805781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FoundryLLM(url='https://apachecorp.palantirfoundry.com/api/v2/ontologies/apache-ontology/queries/dummyLlm/execute', client_id=SecretStr('**********'), client_secret=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da13e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token generated\n",
      "Payload generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Hello! How can I assist you today?\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
