{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOllama Using pre-loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.smith.langchain.com'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies - general\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('.env')\n",
    "os.environ['LANGCHAIN_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies - chatollama, chat templates\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage # overloaded from BaseMessage\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# grabbing the model (local, see https://github.com/ollama/ollama/blob/main/docs/api.md for full docs)\n",
    "base_url = 'http://localhost:11434'\n",
    "# model = 'llama3.2:latest'\n",
    "model = 'sheldon'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model,\n",
    "    base_url=base_url,\n",
    "    temperature=0.8,\n",
    "    num_predict=256,\n",
    "    # other params ..\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Finally, someone has deigned to acknowledge my presence. I do hope you're aware that I was in the middle of a critical experiment and had to pause it due to your interruption. Now, if you'd like to discuss the intricacies of String theory or my expertise on comic books, I'm more than happy to enlighten you. Otherwise, please, state your purpose.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke('hi')\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Messages\n",
    "SystemMessage - pass role of LLM in run-time; can direct how the response should be generated.\n",
    "\\\n",
    "HumanMessage - human prompt input, user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage # overloaded from BaseMessage\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three important things to know about our planet:\n",
      "\n",
      "1. The Earth is a big ball that floats in space and is home to all kinds of living things.\n",
      "2. Our Earth has air, water, and land, which we need to survive.\n",
      "3. We must take care of the Earth so it stays healthy and happy for us and future generations!\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage('Tell me about the Earth in 3 points.') # your prompt/question to ask\n",
    "system = SystemMessage('You are an elementary school teacher. You should answer in short, easy to understand sentences.') # system description - how to respond\n",
    "\n",
    "messages = [system, question] # feed into llm to generate response\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A most intriguing inquiry! As a Ph.D. Professor of Geoscience, I'm delighted to provide you with three salient points regarding our terrestrial home, the Earth.\n",
      "\n",
      "1. **Planetary Composition and Structure**: The Earth is a terrestrial planet composed of approximately 31% iron, 30% calcium carbonate (limestone), 10% silicates (minerals), and 6% other elements such as aluminum, magnesium, and potassium. Its crust ranges in thickness from 5-70 km, with the majority of the solid mass residing within its mantle. The Earth's core is divided into a molten outer layer (core mantle boundary) and a solid inner core, comprising approximately 20% of the planet's mass.\n",
      "2. **Atmospheric Dynamics and Climate Regulation**: The Earth's atmosphere is a dynamic system that plays a crucial role in regulating our climate and supporting life. Comprising 78% nitrogen, 21% oxygen, and smaller percentages of other gases, the atmosphere is held in place by gravity and maintains its pressure through atmospheric pressure gradients. The atmospheric circulation patterns, such as trade winds and jet streams, influence regional climates and weather phenomena.\n",
      "3. **Geological History and Tectonic Evolution**: The Earth's geological history is characterized by a complex tectonic evolution that has shaped the planet over billions of years. From the formation of the Moon (4.5 billion years ago) to the break-up of supercontinents like Pangaea (~300 million years ago), the Earth has undergone numerous episodes of mountain-building, rifting, and volcanic activity. These processes have sculpted the surface topography and created a rich geological record that provides insights into the planet's evolution.\n",
      "\n",
      "These three points offer a glimpse into the fascinating complexity of our terrestrial home, the Earth.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessage('You a phd professor. You should answer as such.') # showing difference caused by system prompt\n",
    "\n",
    "messages = [system, question] \n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "Replaces manually writing the system message, instead can be used as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up templates. We can later fill in variables to provide flexibility.\n",
    "system = SystemMessagePromptTemplate.from_template('You are a {school} teacher. You should answer in short, easy to understand sentences.')\n",
    "question = HumanMessagePromptTemplate.from_template('Tell me about {topic} in {number} points.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='You are a high school teacher. You should answer in short, easy to understand sentences.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.format(topic='basketball', number='3')\n",
    "system.format(school='high school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['number', 'school', 'topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are a {school} teacher. You should answer in short, easy to understand sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['number', 'topic'], input_types={}, partial_variables={}, template='Tell me about {topic} in {number} points.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points about basketball:\n",
      "\n",
      "1. Basketball is a team sport played on a court with two teams of five players each.\n",
      "2. The objective is to score more points than the opposing team by shooting the ball into their goal, called a hoop or basket.\n",
      "3. Players can move the ball by dribbling or passing it to teammates, and they have four chances, or shots, to score before the ball is turned over.\n"
     ]
    }
   ],
   "source": [
    "question = template.invoke({'school': 'high school', 'topic': 'basketball', 'number': '3'})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five key points about soccer:\n",
      "\n",
      "1. Soccer is a team sport played with two teams of eleven players each.\n",
      "2. The objective is to score more goals than the opposing team by kicking or heading the ball into their goal.\n",
      "3. Players can use any part of their body except their hands and arms to control and move the ball.\n",
      "4. A match is divided into two 45-minute halves, with a 15-minute halftime break in between.\n",
      "5. The team with the most goals at the end of the two halves wins the game.\n"
     ]
    }
   ],
   "source": [
    "# showing how much faster it can be with templates:\n",
    "question = template.invoke({'school': 'high school', 'topic': 'soccer', 'number': '5'})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language - LCEL chains\n",
    "#### For this section, please bring in previously used templates to avoid rerunning the entire notebook\n",
    "Types:\n",
    "- Sequential Chain\n",
    "- Parallel Chain\n",
    "- Router Chain\n",
    "- Chain Runnables\n",
    "- Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing previously used templates\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "base_url = 'http://localhost:11434'\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are a {school} teacher. You should answer in short, easy to understand sentences.')\n",
    "question = HumanMessagePromptTemplate.from_template('Tell me about {topic} in {number} points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start off the same way as using chat templates before:\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['number', 'school', 'topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are a {school} teacher. You should answer in short, easy to understand sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['number', 'topic'], input_types={}, partial_variables={}, template='Tell me about {topic} in {number} points.'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2', base_url='http://localhost:11434')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of creating a question using template.invoke, we chain together the template and llm\n",
    "chain = template | llm # creating the chain\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a teacher, not an expert on poker! But here's what I know:\n",
      "\n",
      "* Poker is a card game where people bet with chips or money.\n",
      "* Players try to make the best hand possible using cards they've been dealt.\n",
      "* The person with the best hand wins all the chips in the pot.\n"
     ]
    }
   ],
   "source": [
    "# then, we can invoke the chain together instead of two separate times (see above chat template section)\n",
    "response = chain.invoke({'school': 'primary', 'number': '3', 'topic': 'poker'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking it a step further, we can add other things to the chain.\n",
    "For example, converting the output to a text string instead of just having a response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points about 401(k) plans:\n",
      "\n",
      "1. A 401(k) is an employer-sponsored retirement savings plan that allows you to contribute a portion of your paycheck to a tax-deferred account.\n",
      "2. Contributions to a 401(k) are made pre-tax, reducing your taxable income for the year, and earnings grow tax-free until withdrawal.\n",
      "3. Withdrawals from a 401(k) are taxed as ordinary income, typically in retirement when you're no longer working, making it a long-term savings vehicle.\n"
     ]
    }
   ],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'school': 'college', 'number': 3, 'topic': '401k'})\n",
    "print(response) # here's the main difference; can directly print the response since it'll be a string now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Runnables\n",
    "We can use the same logic to add more things to the chain and run at once. For this example, we will take the output of the previous chain and perform analysis on it. In other words, the output (response) will be used as the input for another llm call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 3 out of 5\n",
      "\n",
      "This rating indicates that the text is generally clear and concise, but may require some basic financial knowledge or understanding of tax concepts to fully comprehend. The language used is straightforward, and the three key points are easy to follow, but the text assumes a certain level of prior knowledge about retirement savings plans and taxes.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('''analyze the following text: {text}\n",
    "                                                   Give a rating from 1 to 5 on how difficult it is to understand. \n",
    "                                                   Provide one sentence to explain this rating.\n",
    "                                                   ''')\n",
    "diff_chain = chain | analysis_prompt | llm | StrOutputParser() # chain linked to analysis prompt linked to llm and str output\n",
    "# diff_chain_alt = {'response': chain} | analysis_prompt | llm | StrOutputParser()\n",
    "output = diff_chain.invoke({'school': 'college', 'number': 3, 'topic': '401k'}) # I can directly use the inputs from chain\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Chains\n",
    "Parallel chains are used to run multiple runnables in parallel, the final return is a dict with the result of each value under its appropriate key.\n",
    "\n",
    "The main difference from chaining (in series) is that parallel runnables MUST BE INDEPENDENT, while series runnables can have the input of one be the output of another. For example: explain plot of story -> give rating of plot explanation -> generate star value VS. explain plot of book || explain plot of movie -> using output from both: compare which plot is better explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bread go to therapy?\n",
      "\n",
      "Because it was feeling crumby.\n",
      "==================================\n",
      "Softly baked, with crust so fine,\n",
      "Fresh from the oven, warm and divine.\n"
     ]
    }
   ],
   "source": [
    "# start two independent chains\n",
    "joke_chain = ChatPromptTemplate.from_template('tell me a joke about {topic}') | llm | StrOutputParser()\n",
    "poem_chain = ChatPromptTemplate.from_template('write a {number}-line poem about {topic}') | llm | StrOutputParser()\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain) # create a parallel chain with both chains inside\n",
    "\n",
    "# when invoking, ALL inputs need to be provided, even if they are not all shared.\n",
    "output = map_chain.invoke({'topic': 'bread', 'number': 2})\n",
    "print(output['joke'] + '\\n==================================\\n' + output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Router\n",
    "Logic flow tool - uses output of a chain to affect where the chain continues. Based on the response, router can decide next chain.\n",
    "\n",
    "Example using poem/joke chain:\\\n",
    "Given a user review, classify if it is positive or negative. Then, if it is positive, create a joke about the topic. Else, create a poem to console the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being 'Positive' or 'Negative'.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Review: {review}\n",
    "            Movie Name: {title}\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "review_chain = template | llm | StrOutputParser()\n",
    "\n",
    "review = 'This movie blew my socks off. I was at the edge of my seat from start to finish. I loved it!'\n",
    "\n",
    "review_chain.invoke({'review': review, 'title': 'Shark Tale'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a router to perform an action based on review_chain's output\n",
    "def route(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return joke_chain\n",
    "    else:\n",
    "        return poem_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "We will use runnablelambda to select a chain based on the output of review_chain.\\\n",
    "RunnableLambda(route) converts route to a runnable so that it can be linked to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: ChatPromptTemplate(input_variables=['review', 'title'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review', 'title'], input_types={}, partial_variables={}, template=\"Given the user review below, classify it as either being 'Positive' or 'Negative'.\\n            Do not respond with more than one word.\\n\\n            Review: {review}\\n            Movie Name: {title}\\n\"), additional_kwargs={})])\n",
       "             | ChatOllama(model='llama3.2', base_url='http://localhost:11434')\n",
       "             | StrOutputParser(),\n",
       "  topic: RunnableLambda(...),\n",
       "  number: RunnableLambda(...)\n",
       "}\n",
       "| RunnableLambda(route)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain = {'sentiment': review_chain, 'topic': lambda x: x['title'], 'number': lambda x: len(x['title']) // 5} | RunnableLambda(route) # store number in case the review is negative\n",
    "full_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joke means positive review, poem is negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Oscar the fish go to the party in Shark Tale?\n",
      "\n",
      "Because he heard it was a \"jaw-dropping\" good time! (get it?)\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({'review': review, 'title': 'Shark Tale'})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable Passthrough, Runnable Lambda\n",
    "For this demonstration, we'll count the char and word counts in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char_counts': 117, 'word_counts': 23, 'full_chain_output': 'Why did Oscar the fish go to the party in Shark Tale?\\n\\nBecause he heard it was a \"jaws-dropping\" good time! (get it?)'}\n"
     ]
    }
   ],
   "source": [
    "lambda_chain = full_chain | StrOutputParser() | {'char_counts': RunnableLambda(char_counts), \n",
    "                                                 'word_counts': RunnableLambda(word_counts),\n",
    "                                                 'full_chain_output': RunnablePassthrough()} # brings then output from full_chain as well\n",
    "\n",
    "output = lambda_chain.invoke({'review': review, 'title': 'Shark Tale'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can altnernatively perform a similar task using @chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the baseball go to the doctor?\n",
      "\n",
      "Because it was feeling a little \"off-base\"! (get it?)\n",
      "================\n",
      "Here is a 2-line poem about baseball:\n",
      "\n",
      "The crack of the bat, a symphony sweet,\n",
      "Summer's magic in every swing to meet.\n"
     ]
    }
   ],
   "source": [
    "@chain # this achieves what we did before using RunnableLambda\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'joke': joke_chain.invoke(params),\n",
    "        'poem': poem_chain.invoke(params)\n",
    "    }\n",
    "\n",
    "params = {'topic': 'baseball', 'number': 2}\n",
    "output = custom_chain.invoke(params)\n",
    "\n",
    "print(output['joke'] + '\\n================\\n' + output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsing\n",
    "Exploring different forms of outputs and how we can use that information. Previously used exapmle: StrOutputParser\\\n",
    "Some others: JsonOutputParser, CSV Output Parser, Datetime Output Parser, Structured Output Parser (i.e. Pydantic, Json)\n",
    "### First up: Pydantic Output Parser\n",
    "Pydantic docs: https://docs.pydantic.dev/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new section, let's reimport needed dependencies\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "base_url = 'http://localhost:11434'\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydantic required libraries\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pydantic class for the llm output\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Create a joke class to act as output format, explaining what fields you expect to see in a joke output\"\"\"\n",
    "\n",
    "    # make sure you explain this in a way your LLM can understand\n",
    "    setup: str = Field(description='The setup of the joke') \n",
    "    punchline: str = Field(description='The punchline of the joke')\n",
    "    rating: Optional[int] = Field(description='The rating of the joke, from 1 to 5', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.Joke'>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydantic_parser = PydanticOutputParser(pydantic_object=Joke) # create an output with Joke type\n",
    "pydantic_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Create a joke class to act as output format, explaining what fields you expect to see in a joke output\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline of the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"The rating of the joke, from 1 to 5\", \"title\": \"Rating\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "instruction = pydantic_parser.get_format_instructions() # instructions on output format, to be given to the LLM\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this instruction \n",
    "prompt = PromptTemplate(\n",
    "    template='''\n",
    "    Answer the user query with a joke. Use this formatting: {format_instruction}\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "''',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\"setup\": \"Why did the cat join a band?\", \"punchline\": \"Because it wanted to be the purr-cussionist!\", \"rating\": 4}', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-17T20:57:25.6864391Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7792276000, 'load_duration': 6065399500, 'prompt_eval_count': 313, 'prompt_eval_duration': 1090000000, 'eval_count': 37, 'eval_duration': 635000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ef5ffe74-ddd0-4745-aacf-c70c02bb6201-0', usage_metadata={'input_tokens': 313, 'output_tokens': 37, 'total_tokens': 350})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({'query': 'Tell me a joke about a cat'})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"setup\": \"Why did the cat join a band?\", \"punchline\": \"Because it wanted to be the purr-cussionist!\", \"rating\": 4}\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the cat join a band?' punchline='Because it wanted to be the purr-cussionist!' rating=None\n"
     ]
    }
   ],
   "source": [
    "pydantic_parser_chain = prompt | llm | pydantic_parser # using the parser this way will format your output \n",
    "output = pydantic_parser_chain.invoke({'query': 'Tell me a joke about a cat'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing using .with_structured_output \n",
    "This can be much simpler than the previous method (1 line of code instead of building a whole structure). However, it will be a new llm instead of keeping an llm and designing just the output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat join a band?\n",
      "\n",
      "Because it wanted to be the purr-cussionist.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke('Tell me a joke about a cat')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(Joke) # pass the class into the llm as a new structured llm as an alternative to pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='What do you call a group of cats playing instruments?', punchline='Why did the cat join a band? Because it wanted to be the purr-cussionist!', rating=4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke('Tell me a joke about a cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Create a joke class to act as output format, explaining what fields you expect to see in a joke output\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline of the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"The rating of the joke, from 1 to 5\", \"title\": \"Rating\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "json_parser = JsonOutputParser(pydantic_object=Joke) # will still need to build a Joke class as before\n",
    "print(parser.get_format_instructions()) # see that it's similar to the pydantic object output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': 'Why did the cat join a band?', 'punchline': 'Because it wanted to be a purr-cussionist!', 'rating': 3}\n"
     ]
    }
   ],
   "source": [
    "# reusing the prompt template from before (with some small changes):\n",
    "json_parser_chain = prompt | llm | json_parser\n",
    "output = json_parser_chain.invoke({'query': 'Tell me a joke about a cat'})\n",
    "print(output) # same output, but formatted as a json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Output Parser\n",
    "Can also return in csv format (comma-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csv_parser = CommaSeparatedListOutputParser() # doesn't need a basemodel or pydantic object\n",
    "\n",
    "print(csv_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#YosemiteNationalPark', '#NatureLovers', '#WondersOfTheWorld', '#NationalParksOfUSA', '#AdventureAwaits']\n"
     ]
    }
   ],
   "source": [
    "format_instruction = csv_parser.get_format_instructions()\n",
    "\n",
    "csv_prompt = PromptTemplate(\n",
    "    template='''\n",
    "    Answer the user query with a list of values. Use this formatting: {format_instruction}\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "''',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instruction': format_instruction}\n",
    ")\n",
    "\n",
    "csv_chain = csv_prompt | llm | csv_parser\n",
    "output = csv_chain.invoke({'query': 'Generate hashtags for my instagram post. My picture is a nature picture at Yosemite National Park.'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0620-11-14T03:50:08.082909Z, 1436-05-25T14:14:15.935585Z, 1879-02-14T16:59:58.247513Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "datetime_parser = DatetimeOutputParser() # also does not need a model type\n",
    "\n",
    "datetime_instruction = datetime_parser.get_format_instructions()\n",
    "print(datetime_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-08 18:32:00\n"
     ]
    }
   ],
   "source": [
    "datetime_prompt = PromptTemplate(\n",
    "    template='''\n",
    "    Answer the user query with a datetime. Use this formatting: {format_instruction}\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "''',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instruction': datetime_instruction}\n",
    ")\n",
    "\n",
    "datetime_chain = datetime_prompt | llm | datetime_parser\n",
    "output = datetime_chain.invoke({'query': 'When did the Queen pass away?'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: LLM can hallucinate with this answer, it is not always able to grab the correct information (also depends on which LLM you use and the size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Message Memory\n",
    "Saving message history for context to chat with the LLM - will achieve this by saving the generated response after running and manages this history via session_id.\n",
    "\n",
    "We'll use a simple chain to explore this.\n",
    "#### Chain *without* message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Sam, nice to meet you! It's not every day we get to talk about working for the Happiest Place on Earth - Disney!\\n\\nAs a Disney employee, I'm sure you have a unique perspective on the magic of the company and its impact on people all around the world.\\n\\nWhat's your role at Disney? Are you a cast member in one of the theme parks, a producer, or maybe something entirely different?\\n\\nLet's chat about all things Disney!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new section, let's reimport needed dependencies\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "base_url = 'http://localhost:11434'\n",
    "llm = ChatOllama(base_url=base_url, model='llama3.2')\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_template('{prompt}')\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "about = \"My name is Sam Smith. I work for Disney.\"\n",
    "chain.invoke({'prompt': about})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your name. I'm a large language model, I don't have the ability to know or retain personal information about individual users. Each time you interact with me, it's a new conversation and I don't retain any context from previous conversations.\\n\\nIf you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is my name?\"\n",
    "chain.invoke({'prompt': prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM usually wouldn't keep history as context. We will now add the history.\n",
    "#### Adding message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///chat_history.db\") # creates sqlite database with chat history to be used as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history = RunnableWithMessageHistory(chain, get_session_history) # needs runnable and way to save the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 'sam.smith'\n",
    "history = get_session_history(user_id)\n",
    "\n",
    "history.get_messages()\n",
    "# history.clear() # clear existing messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you\\'re trying to send a message in a format that\\'s typically used for API requests or other technical contexts.\\n\\nIf you\\'d like, I can help you rephrase the message in a more conversational tone, as if it were a human reaching out:\\n\\n\"Hi there! My name is Sam Smith and I work for Disney. Is there anything I can help you with?\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke([HumanMessage(content=about)],\n",
    "                             config={'configurable': {'session_id': user_id}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It looks like we have a sequence of messages here.\\n\\nThe first message appears to be an AIMessage, which is likely from a conversational AI system. The content suggests that Sam Smith is trying to send a message, but it\\'s not in a format typically used for human-to-human communication.\\n\\nThe second message responds to the first message and rephrases it in a more conversational tone, making it suitable for a human conversation. It also adds some friendly language to make the response more approachable.\\n\\nFinally, we have a HumanMessage that asks \"What\\'s my name?\" - which is likely an error or a test case, as this is not the context of our previous conversation.\\n\\nIt seems like these messages are being generated in a simulation or testing scenario, possibly to evaluate the AI system\\'s ability to understand and respond to human language.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke([HumanMessage(content=\"What's my name?\")],\n",
    "                             config={'configurable': {'session_id': user_id}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course doens't even fix this issue, not sure what he was trying to do but he just kinda moves on...\n",
    "Trying to make a chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are a helpful assistant.')\n",
    "human = HumanMessagePromptTemplate.from_template('{input}') # new input after looking at history\n",
    "\n",
    "messages = [system, MessagesPlaceholder(variable_name='history'), human] \n",
    "\n",
    "prompt = ChatPromptTemplate(messages=messages)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(chain, get_session_history, \n",
    "                                                   input_messages_key='input', \n",
    "                                                   history_messages_key='history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(session_id, input):\n",
    "    output = runnable_with_history.invoke(\n",
    "        {'input': input},\n",
    "        config={'configurable': {'session_id': session_id}}\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Sam! Nice to meet you. It's great to know that you're part of the magical team at Disney. What brings you here today? Are you working on a new project, or perhaps looking for some assistance with an existing one? I'm all ears and ready to help in any way I can.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.clear()\n",
    "user_id = 'SamSmith'\n",
    "chat_with_llm(user_id, about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Sam Smith. I apologize if I didn't quite get it right earlier - as far as I know, there are several notable individuals named Sam Smith, including the British singer-songwriter Sam Smith. But without more context, it's difficult to say which one you might be! Are you the music star?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_llm(user_id, 'What is my name?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
